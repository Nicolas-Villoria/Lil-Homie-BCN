"""
Team11C_exploitation_zone.py
-----------------------
Data Enrichment and Aggregation Module (Silver (Formatted Zone) -> Gold (Exploitation Zone)).

This script constructs the Gold Layer (Exploitation Zone) of the Data Lake.
It orchestrates the final stage of the ETL pipeline by:
1. Reading cleaned data from the Silver Layer (MongoDB) using the Spark Connector.
2. Performing data enrichment by joining Property data with Income and Density metrics via 'neighborhood_id'.
3. Executing feature engineering (e.g., calculating price per mÂ²).
4. Filtering outliers and handling missing values.
5. Writing the final, analytics-ready dataset to Delta Lake.
"""

import os
import sys
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lower, trim, year, month, to_date, when, lit, coalesce, max, avg, broadcast
from delta import *


    
def clean_column_names(df):
    """
    Sanitizes column names for Delta Lake and Parquet compatibility.
    
    Args:
        df (DataFrame): Spark DataFrame with potentially problematic column names.
    
    Returns:
        DataFrame: Same data with sanitized column names.
    """
    for col_name in df.columns:
        new_name = col_name.replace(" ", "_").replace("(", "").replace(")", "").replace("=", "").replace(".", "").strip()
        df = df.withColumnRenamed(col_name, new_name)
    return df


def create_exploitation_zone():
    """
    Creates the Exploitation Zone (Gold Layer) by joining Idealista, Income, and Density data.
    
    Steps:
    1. Initialize Spark with Delta Lake and MongoDB Connector support.
    2. Read Silver data directly from MongoDB into Spark DataFrames.
    3. Clean and normalize schemas (handling mixed types in Density).
    4. Enrich Idealista data by joining with Income and Density on neighborhood.
    5. Perform Feature Engineering (Price/m2) and Outlier Filtering.
    6. Write the final dataset to the Gold Layer (Delta Table).
    """
    
    # Set SPARK_LOCAL_IP to avoid "hostname resolves to loopback" warning
    os.environ['SPARK_LOCAL_IP'] = '127.0.0.1'

    # Initialize Spark with Delta support AND MongoDB Connector
    # We use configure_spark_with_delta_pip to handle Delta dependencies, 
    # and pass the Mongo connector as an extra package.
    
    builder = SparkSession.builder \
        .appName("ExploitationZone") \
        .master("local[*]") \
        .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
        .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
        .config("spark.mongodb.read.connection.uri", "mongodb://localhost:27017/bda_project") \
        .config("spark.mongodb.write.connection.uri", "mongodb://localhost:27017/bda_project")

    # Suppress stderr (fd 2) and stdout (fd 1) to hide Spark/Ivy startup logs
    stderr_fd = sys.stderr.fileno()
    stdout_fd = sys.stdout.fileno()
    saved_stderr_fd = os.dup(stderr_fd)
    saved_stdout_fd = os.dup(stdout_fd)
    
    with open(os.devnull, 'w') as devnull:
        os.dup2(devnull.fileno(), stderr_fd)
        os.dup2(devnull.fileno(), stdout_fd)
        try:
            # Add Mongo connector via extra_packages to ensure it's not overwritten
            # Using 10.3.0 for Spark 3.5 compatibility
            spark = configure_spark_with_delta_pip(
                builder \
                    .config("spark.sql.shuffle.partitions", "8") \
                    .config("spark.sql.adaptive.enabled", "true") \
                    .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
                    .config("spark.default.parallelism", "4"), 
                extra_packages=["org.mongodb.spark:mongo-spark-connector_2.12:10.3.0"]
            ).getOrCreate()
            
            # Set log level to ERROR to suppress warnings
            spark.sparkContext.setLogLevel("ERROR")
        finally:
            os.dup2(saved_stderr_fd, stderr_fd)
            os.dup2(saved_stdout_fd, stdout_fd)
            os.close(saved_stderr_fd)
            os.close(saved_stdout_fd)
    
    # 1. Load Data from Silver Layer (MongoDB) DIRECTLY with Spark
    # This is fully distributed and does not load data into driver memory
    
    print("Reading Idealista from MongoDB (Spark Connector)...")
    df_idealista = spark.read.format("mongodb") \
        .option("database", "bda_project") \
        .option("collection", "idealista") \
        .load()

    print("Reading Income from MongoDB (Spark Connector)...")
    df_income = spark.read.format("mongodb") \
        .option("database", "bda_project") \
        .option("collection", "income") \
        .load()

    print("Reading Density from MongoDB (Spark Connector)...")
    df_density = spark.read.format("mongodb") \
        .option("database", "bda_project") \
        .option("collection", "density") \
        .load()
    
    # --- Pre-processing ---
    
    # 1. Clean Density Data
    df_density_clean = df_density.select(
        col("year").cast("integer"),
        col("neighborhood_id"),
        col("density_val")
    )
    
    # Filter out records with null neighborhood_id, density_val, or year
    df_density_clean = df_density_clean.filter(
        col("neighborhood_id").isNotNull() & col("density_val").isNotNull() & col("year").isNotNull()
    )
    
    # Deduplicate by neighborhood and year (take max density if duplicates)
    df_density_clean = df_density_clean.groupBy("neighborhood_id", "year") \
        .agg(max("density_val").alias("density_val"))
    
    # Cache small dimension table for efficient joins 
    df_density_clean.cache()

    # 2. Clean Income Data
    # Filter out records with null neighborhood_id or year
    df_income_clean = df_income.filter(
        col("neighborhood_id").isNotNull() & col("year").isNotNull()
    )
    
    # Group by neighborhood AND year to get temporal income profile
    df_income_agg = df_income_clean.groupBy("neighborhood_id", "year") \
        .agg(avg("income").alias("avg_income_index"))

    # 3. Clean Idealista Data
    # Filter to only municipality of Barcelona 
    df_idealista_clean = df_idealista.filter(col("municipality") == "Barcelona")
    
    # Also filter out records with null neighborhood_id (unmapped neighborhoods)
    df_idealista_clean = df_idealista_clean.filter(col("neighborhood_id").isNotNull())
    
    # Extract year from data_year column (cast to integer for joining)
    df_idealista_clean = df_idealista_clean.withColumn("year", col("data_year").cast("integer"))
    
    # Filter out records with null year
    df_idealista_clean = df_idealista_clean.filter(col("year").isNotNull())
    
    # Drop constant or useless columns for ML
    # municipality: Constant "Barcelona"
    # property_id: High cardinality ID (useless for pattern recognition)
    df_idealista_clean = df_idealista_clean.drop("municipality", "property_id")

    # Clean Column Names to ensure compatibility
    df_idealista_clean = clean_column_names(df_idealista_clean)
    df_income_agg = clean_column_names(df_income_agg)
    df_density_clean = clean_column_names(df_density_clean)

    # 4. Perform Joins 
    # Join Strategy: Use neighborhood_id AND year for TEMPORAL consistency
    # Why: Property prices in 2020 should match with 2020 income/density data
    # Inner join: Drops properties without matching socioeconomic data (data quality filter)
    # Broadcast small Income table (~200 rows) to avoid shuffle on large Idealista table
    df_joined = df_idealista_clean.join(broadcast(df_income_agg), on=["neighborhood_id", "year"], how="inner")
    
    # Join with Density using neighborhood_id AND year
    # Broadcast small cached Density table (~73 neighborhoods) to avoid shuffle
    df_joined = df_joined.join(broadcast(df_density_clean), on=["neighborhood_id", "year"], how="left")
    
    # Drop the join key and redundant name columns 
    cols_to_drop = [c for c in ["join_key", "neighborhood_name"] if c in df_joined.columns]
    if cols_to_drop:
        df_joined = df_joined.drop(*cols_to_drop)
    
    # Cache joined dataframe before expensive operations (IQR, filtering)
    # This prevents re-reading from MongoDB if job fails during feature engineering
    df_joined = df_joined.cache()
    initial_count = df_joined.count()  # Materialize cache
    print(f"Joined dataset cached: {initial_count} records")
    
    # --- Feature Engineering & Filtering ---
    
    # Calculate Price per m2
    df_final = df_joined.withColumn("price_m2", col("price") / col("size"))
    
    # 1. Logical Filters (Sanity Checks)
    # Size > 15 m2 (Cedula de habitabilidad usually requires min size)
    # Price > 10,000 (Assuming sales data, nothing sells for less)
    df_final = df_final.filter((col("size") > 15) & (col("price") > 10000))

    # 2. Statistical Outlier Detection using Interquartile Range (IQR) method
    # Why IQR: More robust than standard deviation for skewed distributions
    # This adapts to actual market distribution rather than assuming normality
    print("Calculating IQR thresholds for outlier detection...")
    
    # Calculate Q1 (25th percentile) and Q3 (75th percentile)
    quantiles = df_final.stat.approxQuantile("price_m2", [0.25, 0.75], 0.01)
    Q1 = quantiles[0]
    Q3 = quantiles[1]
    IQR = Q3 - Q1
    
    # Define bounds (Standard is 1.5 * IQR)
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    
    print(f"IQR Bounds for Price/m2: {lower_bound:.2f} - {upper_bound:.2f} (IQR={IQR:.2f})")
    
    # Apply Filter
    df_final = df_final.filter((col("price_m2") >= lower_bound) & (col("price_m2") <= upper_bound))
    
    final_count = df_final.count()
    print(f"Filtered {initial_count - final_count} statistical outliers.")
    
    # Handle Nulls (Imputation)
    # Fill missing income/density with mean (using 0 for simplicity as per previous logic)
    # For hasLift, we assume missing means False (feature not present/not listed)
    df_final = df_final.fillna(df_final.agg({"avg_income_index": "mean"}).collect()[0][0], subset=["avg_income_index"]) \
                       .fillna(df_final.agg({"density_val": "mean"}).collect()[0][0], subset=["density_val"]) \
                       .fillna(False, subset=["hasLift"])
    
    # Drop unnecessary columns before writing
    df_final = df_final.drop("_id", "data_year", "year", "neighborhood_id")
    
    # --- Write to Gold Layer (Delta Lake) ---
    gold_path = "data_lake/gold/property_prices"
    
    print(f"Writing {final_count} records to Gold Layer at {gold_path}...")
    
    # Add execution timestamp for partitioning (best practice for Airflow pipelines)
    from datetime import datetime
    execution_date = datetime.now().strftime("%Y-%m-%d")
    df_final = df_final.withColumn("pipeline_execution_date", lit(execution_date))
    
    # Write partitioned by execution date (avoids concurrent write conflicts)
    # First time: overwrite to create partitioned table structure
    # Subsequent runs: append mode will work automatically
    df_final.write.format("delta") \
        .mode("overwrite") \
        .partitionBy("pipeline_execution_date") \
        .option("overwriteSchema", "true") \
        .save(gold_path)
    
    # Save latest execution as CSV for quick data exploration (parallel write)
    csv_output_path = f"data_lake/gold/property_prices_csv/{execution_date}"
    df_final.drop("pipeline_execution_date") \
        .write.mode("overwrite").option("header", "true").csv(csv_output_path)
    print(f"Latest execution saved to {csv_output_path}")
        
    print("Exploitation Zone created successfully.")
    
if __name__ == "__main__":
    create_exploitation_zone()
